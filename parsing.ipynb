{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install fake_useragent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_2nq7P6krPq",
        "outputId": "8d5e68ed-7aa9-45e3-e1de-5114421acb67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-1.4.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: fake_useragent\n",
            "Successfully installed fake_useragent-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OTsV6jwjCj2",
        "outputId": "9a977d65-14d9-40a1-a35b-cb02e0b66c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page:  0 , status_code:  200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84% (22 of 26) |####################    | Elapsed Time: 0:00:51 ETA:   0:00:09"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ошибка при запросе: 'NoneType' object has no attribute 'get_text'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% (26 of 26) |########################| Elapsed Time: 0:01:01 ETA:  00:00:00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page:  0 , status_code:  200\n",
            "page:  1 , status_code:  200\n",
            "page:  2 , status_code:  200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13% (35 of 269) |##                     | Elapsed Time: 0:01:24 ETA:   0:08:59"
          ]
        }
      ],
      "source": [
        "from os import name\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import progressbar\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import fake_useragent\n",
        "import time\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "name_vacancies = ['\"ML-инженер\" OR \"Data scientist\" OR \"Data engineer\"', \"Тестировщик\", \"Бизнес аналитик\", \"DevOps инженер\", \"IOS разработчик\", \"Android разработчик\", '\"продуктовый аналитик\" OR \"product analyst\"']\n",
        "for item in name_vacancies:\n",
        "  url = 'https://api.hh.ru/vacancies'\n",
        "\n",
        "  params = {\n",
        "            'text': item,\n",
        "            'per_page': 100,\n",
        "            'page': 0,\n",
        "            'area': 113,\n",
        "            'currency': 'RUR',\n",
        "            'date_from': '2023-06-01',\n",
        "            'date_to': '2023-12-31',\n",
        "            'only_with_salary': 'True'\n",
        "            }\n",
        "\n",
        "  resp = requests.get(url = url, params = params)\n",
        "  answer = resp.text\n",
        "  json_text = json.loads(answer)\n",
        "  items = []\n",
        "  count_pages = json_text['pages']\n",
        "  for i in range(count_pages):\n",
        "    params['page'] = i\n",
        "    resp = requests.get(url=url, params=params)\n",
        "    print('page: ', i, ', status_code: ', resp.status_code)\n",
        "    json_text = json.loads(resp.text)\n",
        "    items.extend(json_text['items'])\n",
        "  # df = pd.DataFrame.from_dict(items, orient='columns')\n",
        "  df = pd.json_normalize(items)\n",
        "  prof_role_dict = pd.json_normalize(df['professional_roles'])\n",
        "  prof_role_dict.columns = ['name']\n",
        "  prof_role_name = pd.json_normalize(prof_role_dict['name'])\n",
        "  prof_role_name.rename(columns={'id': 'professional_roles.id', 'name': 'professional_roles.name'}, inplace=True)\n",
        "  # working_time_modes = pd.json_normalize(df['working_time_modes'])\n",
        "  # working_time_modes.columns = ['working_time_modes']\n",
        "  # working_time_modes = pd.json_normalize(working_time_modes['working_time_modes'])\n",
        "  # working_time_modes.rename(columns={'id': 'working_time_modes.id', 'name': 'working_time_modes.name'}, inplace=True)\n",
        "  df['working_time_modes.id'] = 'NaN'\n",
        "  # df['working_time_modes.name'] = 'NaN'\n",
        "  df['salary'] = 'NaN'\n",
        "  # working_time_intervals = pd.json_normalize(df['working_time_intervals'])\n",
        "  # working_time_intervals.columns = ['working_time_intervals']\n",
        "  # working_time_intervals = pd.json_normalize(working_time_intervals['working_time_intervals'])\n",
        "  # working_time_intervals.rename(columns={'id': 'working_time_intervals.id', 'name': 'working_time_intervals.name'}, inplace=True)\n",
        "  df_combined = pd.concat([df, prof_role_name], axis=1)\n",
        "  df_selected = df_combined[['name', 'id', 'premium', 'alternate_url', 'created_at','employer.name', 'area.name','professional_roles.name', 'employment.name', 'experience.name', 'salary', 'salary.from','salary.to', 'salary.currency', 'salary.gross', 'schedule.name', 'snippet.requirement', 'snippet.responsibility', 'type.name']]\n",
        "\n",
        "  bar = progressbar.ProgressBar(max_value=len(df_selected['id']))\n",
        "\n",
        "  def get_links(id):\n",
        "    try:\n",
        "      usag = fake_useragent.UserAgent()\n",
        "      data = requests.get(url = f\"https://hh.ru/vacancy/{id}\",\n",
        "                      # \"https://hh.ru/vacancy/88796932\",\n",
        "                      headers = {\"user-agent\":usag.random})\n",
        "      resp = data.content\n",
        "      # resp.raise_for_status()\n",
        "      b = BeautifulSoup(resp, 'lxml')\n",
        "      vacancy_description = b.find('div', attrs={\"data-qa\":\"vacancy-description\"}).get_text()\n",
        "    except Exception as e:\n",
        "      print(f\"Ошибка при запросе: {e}\")\n",
        "      # continue\n",
        "\n",
        "    else:\n",
        "      return vacancy_description\n",
        "\n",
        "  descriptions = []\n",
        "\n",
        "  # if __name__==\"__main__\":\n",
        "  for i, id in enumerate(list(df_selected['id'])):\n",
        "    if pd.isna(id) or id == \"\" or id == \"NA\":\n",
        "      continue\n",
        "    else:\n",
        "      time.sleep(1)\n",
        "      descriptions.append([id, get_links(id)])\n",
        "      res = pd.DataFrame(np.asarray(descriptions), columns = ['id', 'description.vacancy'])\n",
        "      bar.update(i+1)\n",
        "\n",
        "  merged_df = df_selected.merge(res, on='id')\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "merged_df.to_csv(f'/content/drive/MyDrive/Project/{item}.csv', index=False)\n"
      ]
    }
  ]
}